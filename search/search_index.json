{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GitOps Docs \u00b6 Welcome to the docs on Kubernetes cluster. Work in progress This document is a work in progress.","title":"Introduction"},{"location":"#gitops-docs","text":"Welcome to the docs on Kubernetes cluster. Work in progress This document is a work in progress.","title":"GitOps Docs"},{"location":"flux/","text":"Flux \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install fluxcd/tap/flux Install the cluster components \u00b6 For full installation guide visit the Flux installation guide Set the GITHUB_TOKEN environment variable to a personal access token you created in Github. export GITHUB_TOKEN = 47241b5a1f9cc45cc7388cf787fc6abacf99eb70 Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster flux bootstrap github \\ --version = v0.9.0 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster \\ --personal \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work Useful commands \u00b6 Force flux to sync your repository: flux reconcile source git flux-system Force flux to sync a helm release: flux reconcile helmrelease sonarr -n default Force flux to sync a helm repository: flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Flux"},{"location":"flux/#flux","text":"Work in progress This document is a work in progress.","title":"Flux"},{"location":"flux/#install-the-cli-tool","text":"brew install fluxcd/tap/flux","title":"Install the CLI tool"},{"location":"flux/#install-the-cluster-components","text":"For full installation guide visit the Flux installation guide Set the GITHUB_TOKEN environment variable to a personal access token you created in Github. export GITHUB_TOKEN = 47241b5a1f9cc45cc7388cf787fc6abacf99eb70 Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster flux bootstrap github \\ --version = v0.9.0 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster \\ --personal \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work","title":"Install the cluster components"},{"location":"flux/#useful-commands","text":"Force flux to sync your repository: flux reconcile source git flux-system Force flux to sync a helm release: flux reconcile helmrelease sonarr -n default Force flux to sync a helm repository: flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Useful commands"},{"location":"git/","text":"Git \u00b6 Work in progress This document is a work in progress. clone git repository with specific revision. \u00b6 $ git clone $URL $ cd $PROJECT_NAME $ git reset --hard $SHA1 To again go back to the most recent commit $ git pull","title":"Github"},{"location":"git/#git","text":"Work in progress This document is a work in progress.","title":"Git"},{"location":"git/#clone-git-repository-with-specific-revision","text":"$ git clone $URL $ cd $PROJECT_NAME $ git reset --hard $SHA1 To again go back to the most recent commit $ git pull","title":"clone git repository with specific revision."},{"location":"helmrelease/","text":"HelmRelease \u00b6 Work in progress This document is a work in progress.","title":"HelmRelease"},{"location":"helmrelease/#helmrelease","text":"Work in progress This document is a work in progress.","title":"HelmRelease"},{"location":"aws/ec2/","text":"EC2 \u00b6 Work in progress This document is a work in progress.","title":"ec2"},{"location":"aws/ec2/#ec2","text":"Work in progress This document is a work in progress.","title":"EC2"},{"location":"aws/eks/","text":"EKS \u00b6 Work in progress This document is a work in progress. Pod networking (CNI) \u00b6 Amazon EKS supports native VPC networking via the Amazon VPC CNI plugin for Kubernetes. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. From https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html This is really a great feature. But there is a one import limitation we will encounter if you want to run so many Pods in a EC2 worker node. Whenever you deploy a Pod in the EKS worker node, EKS creates a new IP address from VPC subnet and attach to the instance. You can check how many ethernet interfaces and max IP per interface from https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI Instance type Maximum network interfaces Private IPv4 addresses per interface IPv6 addresses per interface m4.large 2 10 10 m4.xlarge 4 15 15 m4.2xlarge 4 15 15 m4.4xlarge 8 30 30 m4.10xlarge 8 30 30 m4.16xlarge 8 30 30 m5.large 3 10 10 m5.xlarge 4 15 15 m5.2xlarge 4 15 15 Here is the formula for max Pods numbers. Max Pods = Maximum supported Network Interfaces for instance type ) * ( IPv4 Addresses per Interface ) - 1 For example, if you have a t3.medium instance which support max 3 ethernets and 6 IPs per interface. You can create only 17 pods including the kubernetes internal Pods, Because One IP is reserved for nodes itself. 3 * 6 - 1 = 17","title":"eks"},{"location":"aws/eks/#eks","text":"Work in progress This document is a work in progress.","title":"EKS"},{"location":"aws/eks/#pod-networking-cni","text":"Amazon EKS supports native VPC networking via the Amazon VPC CNI plugin for Kubernetes. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. From https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html This is really a great feature. But there is a one import limitation we will encounter if you want to run so many Pods in a EC2 worker node. Whenever you deploy a Pod in the EKS worker node, EKS creates a new IP address from VPC subnet and attach to the instance. You can check how many ethernet interfaces and max IP per interface from https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI Instance type Maximum network interfaces Private IPv4 addresses per interface IPv6 addresses per interface m4.large 2 10 10 m4.xlarge 4 15 15 m4.2xlarge 4 15 15 m4.4xlarge 8 30 30 m4.10xlarge 8 30 30 m4.16xlarge 8 30 30 m5.large 3 10 10 m5.xlarge 4 15 15 m5.2xlarge 4 15 15 Here is the formula for max Pods numbers. Max Pods = Maximum supported Network Interfaces for instance type ) * ( IPv4 Addresses per Interface ) - 1 For example, if you have a t3.medium instance which support max 3 ethernets and 6 IPs per interface. You can create only 17 pods including the kubernetes internal Pods, Because One IP is reserved for nodes itself. 3 * 6 - 1 = 17","title":"Pod networking (CNI)"},{"location":"cache/redis/","text":"Work in progress This document is a work in progress.","title":"Redis"},{"location":"databases/mongodb/","text":"Work in progress This document is a work in progress.","title":"MongoDB"},{"location":"databases/mysql/","text":"Work in progress This document is a work in progress.","title":"MySQL"},{"location":"databases/postgresql/","text":"Work in progress This document is a work in progress.","title":"PostgreSQL"},{"location":"docker/logging/","text":"Logging \u00b6 Work in progress This document is a work in progress. Understanding Docker Logging and Log Files \u00b6 Docker Logging Commands docker logs is a command that shows all the information logged by a running container. The docker service logs command shows information logged by all the containers participating in a service. By default, the output of these commands, as it would appear if you run the command in a terminal, opens up three I/O streams: stdin, stdout, and stderr . And the default is set to show only stdout and stdout . stdin is the command\u2019s input stream, which may include input from the keyboard or input from another command. stdout is usually a command\u2019s normal output. stderr is typically used to output error messages. The docker logs command may not be useful in cases when a logging driver is configured to send logs to a file, database, or an external host/backend, or if the image is configured to send logs to a file instead of stdout and stderr. With docker logs <CONTAINER_ID> , you can see all the logs broadcast by a specific container identified by a unique ID $ docker run --name test -d busybox sh -c \"while true; do $(echo date); sleep 1; done\" $ date Tue 06 Feb 2020 00:00:00 UTC $ docker logs -f --until=2s test Tue Aug 24 13:41:55 UTC 2021 Tue Aug 24 13:41:56 UTC 2021 Tue Aug 24 13:41:57 UTC 2021 Tue Aug 24 13:41:58 UTC 2021 Tue Aug 24 13:41:59 UTC 2021 docker logs works a bit differently in community and enterprise versions. In Docker Enterprise, docker logs read logs created by any logging driver, but in Docker CE, it can only read logs created by the json-file , local, and journald drivers. For example, here\u2019s the JSON log created by the hello-world Docker image using the json-file driver: {\"log\":\"Hello from Docker!\\n\",\"stream\":\"stdout\",\"time\":\"2021-02-10T00:00:00.000000000Z\"} As you can see, the log follows a pattern of printing: Log\u2019s origin Either stdout or stderr A timestamp You can find this log in your Docker host at: /var/lib/docker/containers/<container id>/<container id>-json.log Logging Drivers Supported by Docker \u00b6 Logging drivers, or log-drivers, are mechanisms for getting information from running containers and services, and Docker has lots of them available for you. Every Docker daemon has a default logging driver, which each container uses. Docker uses the json-file logging driver as its default driver. Currently, Docker supports the following logging drivers: Driver Description none No logs are available for the container and Docker logs do not return any output. local Logs are stored in a custom format designed for minimal overhead. json-file Logs are formatted as JSON. The default logging driver for Docker. syslog Writes logging messages to the Syslog facility. The Syslog daemon must be running on the host machine. journald Writes log messages to journald. The journald daemon must be running on the host machine. gcplogs Writes log messages to Google Cloud Platform (GCP) logging. awslogs Writes log messages to Amazon CloudWatch logs. splunk Writes log messages to Splunk using the HTTP Event Collector. gelf Writes log messages to a Graylog Extended Log Format (GELF) endpoint, such as Graylog or Logstash. fluentd Writes log messages to fluentd (forward input). The fluentd daemon must be running on the host machine. etwlogs Writes log messages as Event Tracing for Windows (ETW) events. Only available on Windows platforms. logentries Writes log messages to Rapid7 Logentries. Configuring the Logging Driver \u00b6 To configure the Docker daemon to a logging driver, you need to set the value of log-driver to the name of the logging driver in the daemon.json configuration file. Then you need to restart Docker for the changes to take effect for all the newly created containers. All the existing containers will remain as they are. For example, let\u2019s set up a default logging driver with some additional options. { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"25m\", \"max-file\": \"10\", \"labels\": \"production_bind\", \"env\": \"os,customer\" } } To find the current logging driver for the Docker daemon: $ docker info --format '{{.LoggingDriver}}' json-file docker-compose with logging \u00b6 version: '3' services: logger: image: nginx container_name: \"logger\" hostname: \"logger\" restart: always logging: driver: json-file options: max-size: \"10m\" max-file: \"5\" environment: - LOG_FILES=true - LOG_SYSLOG=false - MAX_FILES=10 - MAX_SIZE=50 - MAX_AGE=20 - DEBUG=false volumes: - ./logs:/srv/logs https://earthly.dev/blog/understanding-docker-logging-and-log-files/","title":"logging"},{"location":"docker/logging/#logging","text":"Work in progress This document is a work in progress.","title":"Logging"},{"location":"docker/logging/#understanding-docker-logging-and-log-files","text":"Docker Logging Commands docker logs is a command that shows all the information logged by a running container. The docker service logs command shows information logged by all the containers participating in a service. By default, the output of these commands, as it would appear if you run the command in a terminal, opens up three I/O streams: stdin, stdout, and stderr . And the default is set to show only stdout and stdout . stdin is the command\u2019s input stream, which may include input from the keyboard or input from another command. stdout is usually a command\u2019s normal output. stderr is typically used to output error messages. The docker logs command may not be useful in cases when a logging driver is configured to send logs to a file, database, or an external host/backend, or if the image is configured to send logs to a file instead of stdout and stderr. With docker logs <CONTAINER_ID> , you can see all the logs broadcast by a specific container identified by a unique ID $ docker run --name test -d busybox sh -c \"while true; do $(echo date); sleep 1; done\" $ date Tue 06 Feb 2020 00:00:00 UTC $ docker logs -f --until=2s test Tue Aug 24 13:41:55 UTC 2021 Tue Aug 24 13:41:56 UTC 2021 Tue Aug 24 13:41:57 UTC 2021 Tue Aug 24 13:41:58 UTC 2021 Tue Aug 24 13:41:59 UTC 2021 docker logs works a bit differently in community and enterprise versions. In Docker Enterprise, docker logs read logs created by any logging driver, but in Docker CE, it can only read logs created by the json-file , local, and journald drivers. For example, here\u2019s the JSON log created by the hello-world Docker image using the json-file driver: {\"log\":\"Hello from Docker!\\n\",\"stream\":\"stdout\",\"time\":\"2021-02-10T00:00:00.000000000Z\"} As you can see, the log follows a pattern of printing: Log\u2019s origin Either stdout or stderr A timestamp You can find this log in your Docker host at: /var/lib/docker/containers/<container id>/<container id>-json.log","title":"Understanding Docker Logging and Log Files"},{"location":"docker/logging/#logging-drivers-supported-by-docker","text":"Logging drivers, or log-drivers, are mechanisms for getting information from running containers and services, and Docker has lots of them available for you. Every Docker daemon has a default logging driver, which each container uses. Docker uses the json-file logging driver as its default driver. Currently, Docker supports the following logging drivers: Driver Description none No logs are available for the container and Docker logs do not return any output. local Logs are stored in a custom format designed for minimal overhead. json-file Logs are formatted as JSON. The default logging driver for Docker. syslog Writes logging messages to the Syslog facility. The Syslog daemon must be running on the host machine. journald Writes log messages to journald. The journald daemon must be running on the host machine. gcplogs Writes log messages to Google Cloud Platform (GCP) logging. awslogs Writes log messages to Amazon CloudWatch logs. splunk Writes log messages to Splunk using the HTTP Event Collector. gelf Writes log messages to a Graylog Extended Log Format (GELF) endpoint, such as Graylog or Logstash. fluentd Writes log messages to fluentd (forward input). The fluentd daemon must be running on the host machine. etwlogs Writes log messages as Event Tracing for Windows (ETW) events. Only available on Windows platforms. logentries Writes log messages to Rapid7 Logentries.","title":"Logging Drivers Supported by Docker"},{"location":"docker/logging/#configuring-the-logging-driver","text":"To configure the Docker daemon to a logging driver, you need to set the value of log-driver to the name of the logging driver in the daemon.json configuration file. Then you need to restart Docker for the changes to take effect for all the newly created containers. All the existing containers will remain as they are. For example, let\u2019s set up a default logging driver with some additional options. { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"25m\", \"max-file\": \"10\", \"labels\": \"production_bind\", \"env\": \"os,customer\" } } To find the current logging driver for the Docker daemon: $ docker info --format '{{.LoggingDriver}}' json-file","title":"Configuring the Logging Driver"},{"location":"docker/logging/#docker-compose-with-logging","text":"version: '3' services: logger: image: nginx container_name: \"logger\" hostname: \"logger\" restart: always logging: driver: json-file options: max-size: \"10m\" max-file: \"5\" environment: - LOG_FILES=true - LOG_SYSLOG=false - MAX_FILES=10 - MAX_SIZE=50 - MAX_AGE=20 - DEBUG=false volumes: - ./logs:/srv/logs https://earthly.dev/blog/understanding-docker-logging-and-log-files/","title":"docker-compose with logging"},{"location":"docker/volume/","text":"Volume \u00b6 Work in progress This document is a work in progress.","title":"volume"},{"location":"docker/volume/#volume","text":"Work in progress This document is a work in progress.","title":"Volume"},{"location":"gcp/vps/","text":"Work in progress This document is a work in progress.","title":"vm"},{"location":"gitops/argo/","text":"Work in progress This document is a work in progress.","title":"Argo"},{"location":"gitops/fluxcd/","text":"Work in progress This document is a work in progress.","title":"Fluxcd"},{"location":"helm/tips/","text":"Work in progress This document is a work in progress.","title":"Tips"},{"location":"k8s/deployment/","text":"deployment \u00b6 Work in progress This document is a work in progress. Liveness probes \u00b6 Liveness probes are used in Kubernetes to know when a pod is alive or dead. A pod can be in a dead state for a variety of reasons; Kubernetes will kill and recreate the pod when a liveness probe does not pass. Readiness probes \u00b6 Readiness probes are used in Kubernetes to know when a pod is ready to serve traffic. Only when the readiness probe passes will a pod receive traffic from the service; if a readiness probe fails traffic will not be sent to the pod.","title":"deployment"},{"location":"k8s/deployment/#deployment","text":"Work in progress This document is a work in progress.","title":"deployment"},{"location":"k8s/deployment/#liveness-probes","text":"Liveness probes are used in Kubernetes to know when a pod is alive or dead. A pod can be in a dead state for a variety of reasons; Kubernetes will kill and recreate the pod when a liveness probe does not pass.","title":"Liveness probes"},{"location":"k8s/deployment/#readiness-probes","text":"Readiness probes are used in Kubernetes to know when a pod is ready to serve traffic. Only when the readiness probe passes will a pod receive traffic from the service; if a readiness probe fails traffic will not be sent to the pod.","title":"Readiness probes"},{"location":"k8s/networking/","text":"","title":"networking"},{"location":"k8s/service/","text":"service \u00b6 Work in progress This document is a work in progress.","title":"service"},{"location":"k8s/service/#service","text":"Work in progress This document is a work in progress.","title":"service"},{"location":"kafka/Debezium/","text":"Debezium \u00b6 Work in progress This document is a work in progress.","title":"Debezium"},{"location":"kafka/Debezium/#debezium","text":"Work in progress This document is a work in progress.","title":"Debezium"},{"location":"kafka/cheat-sheet/","text":"Work in progress This document is a work in progress. Kafka Topics \u00b6 List existing topics \u00b6 bin/kafka-topics.sh --zookeeper localhost:2181 --list Describe a topic \u00b6 bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic mytopic Purge a topic \u00b6 bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic mytopic --config retention.ms=1000 ... wait a minute ... bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic mytopic --delete-config retention.ms Delete a topic \u00b6 bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic mytopic Get number of messages in a topic ??? \u00b6 bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic mytopic --time -1 --offsets 1 | awk -F \":\" '{sum += $3} END {print sum}' Get the earliest offset still in a topic \u00b6 bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic mytopic --time -2 Get the latest offset still in a topic \u00b6 bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic mytopic --time -1 Consume messages with the console consumer \u00b6 bin/kafka-console-consumer.sh --new-consumer --bootstrap-server localhost:9092 --topic mytopic --from-beginning Get the consumer offsets for a topic \u00b6 bin/kafka-consumer-offset-checker.sh --zookeeper=localhost:2181 --topic=mytopic --group=my_consumer_group Read from __consumer_offsets \u00b6 Add the following property to config/consumer.properties : exclude.internal.topics=false bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --from-beginning --topic __consumer_offsets --zookeeper localhost:2181 --formatter \"kafka.coordinator.GroupMetadataManager\\$OffsetsMessageFormatter\" Kafka Consumer Groups \u00b6 List the consumer groups known to Kafka \u00b6 bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --list (old api) bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --list (new api) View the details of a consumer group \u00b6 bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --describe --group <group name> Kafkacat \u00b6 Getting the last five message of a topic \u00b6 kafkacat -C -b localhost:9092 -t mytopic -p 0 -o -5 -e Zookeeper \u00b6 Starting the Zookeeper Shell \u00b6 bin/zookeeper-shell.sh localhost:2181","title":"CheatSheet"},{"location":"kafka/cheat-sheet/#kafka-topics","text":"","title":"Kafka Topics"},{"location":"kafka/cheat-sheet/#list-existing-topics","text":"bin/kafka-topics.sh --zookeeper localhost:2181 --list","title":"List existing topics"},{"location":"kafka/cheat-sheet/#describe-a-topic","text":"bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic mytopic","title":"Describe a topic"},{"location":"kafka/cheat-sheet/#purge-a-topic","text":"bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic mytopic --config retention.ms=1000 ... wait a minute ... bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic mytopic --delete-config retention.ms","title":"Purge a topic"},{"location":"kafka/cheat-sheet/#delete-a-topic","text":"bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic mytopic","title":"Delete a topic"},{"location":"kafka/cheat-sheet/#get-number-of-messages-in-a-topic","text":"bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic mytopic --time -1 --offsets 1 | awk -F \":\" '{sum += $3} END {print sum}'","title":"Get number of messages in a topic ???"},{"location":"kafka/cheat-sheet/#get-the-earliest-offset-still-in-a-topic","text":"bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic mytopic --time -2","title":"Get the earliest offset still in a topic"},{"location":"kafka/cheat-sheet/#get-the-latest-offset-still-in-a-topic","text":"bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic mytopic --time -1","title":"Get the latest offset still in a topic"},{"location":"kafka/cheat-sheet/#consume-messages-with-the-console-consumer","text":"bin/kafka-console-consumer.sh --new-consumer --bootstrap-server localhost:9092 --topic mytopic --from-beginning","title":"Consume messages with the console consumer"},{"location":"kafka/cheat-sheet/#get-the-consumer-offsets-for-a-topic","text":"bin/kafka-consumer-offset-checker.sh --zookeeper=localhost:2181 --topic=mytopic --group=my_consumer_group","title":"Get the consumer offsets for a topic"},{"location":"kafka/cheat-sheet/#read-from-__consumer_offsets","text":"Add the following property to config/consumer.properties : exclude.internal.topics=false bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --from-beginning --topic __consumer_offsets --zookeeper localhost:2181 --formatter \"kafka.coordinator.GroupMetadataManager\\$OffsetsMessageFormatter\"","title":"Read from __consumer_offsets"},{"location":"kafka/cheat-sheet/#kafka-consumer-groups","text":"","title":"Kafka Consumer Groups"},{"location":"kafka/cheat-sheet/#list-the-consumer-groups-known-to-kafka","text":"bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --list (old api) bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --list (new api)","title":"List the consumer groups known to Kafka"},{"location":"kafka/cheat-sheet/#view-the-details-of-a-consumer-group","text":"bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --describe --group <group name>","title":"View the details of a consumer group"},{"location":"kafka/cheat-sheet/#kafkacat","text":"","title":"Kafkacat"},{"location":"kafka/cheat-sheet/#getting-the-last-five-message-of-a-topic","text":"kafkacat -C -b localhost:9092 -t mytopic -p 0 -o -5 -e","title":"Getting the last five message of a topic"},{"location":"kafka/cheat-sheet/#zookeeper","text":"","title":"Zookeeper"},{"location":"kafka/cheat-sheet/#starting-the-zookeeper-shell","text":"bin/zookeeper-shell.sh localhost:2181","title":"Starting the Zookeeper Shell"},{"location":"kafka/kafka/","text":"Work in progress This document is a work in progress. Install kafka without zookeeper \u00b6 In this article we will see how kafka can be setup without using zookeeper. We will setup a 3 node kafka cluster and create a test topic. We will use a kafka producer to produce data into the test topic and also use a kafka consumer to consume data from the kafka topic. Why is zookeeper used in kafka \u00b6 Zookeeper is used to store kafka cluster metadata information. Zookeeper stores information like topic configuration, topic partition locations and so on. How will kafka work without zookeeper \u00b6 Zookeeper acts as a external metadata management system for kafka. This creates multiple problems such as data duplication, increased system complexity and also leads to additional java processes being used. In order to run kafka without zookeeper, it can be run using Kafka Raft metadata mode ( KRaft ). In KRaft the kafka metadata information will be stored as a partition within kafka itself. There will be a KRaft Quorum of controller nodes which will be used to store the metadata. The metadata will be stored in an internal kafka topic @metadata . This is available in an experimental mode in kafka 2.8.0 Right now using KRaft is experimental and should not be used in production Setting up a kafka cluster without zookeeper Download kafka Download kafka 2.8.0 from https://kafka.apache.org/downloads wget https://apachemirror.wuchna.com/kafka/2.8.0/kafka_2.12-2.8.0.tgz Extract kafka tar xzf kafka_2.12-2.8.0.tgz Open the kafka folder. All kafka commands should be run in the kafka folder cd kafka_2.12-2.8.0 Kafka cluster configuration If you go to config/kraft folder inside the kafka home directory, you will see a file called server.properties. This is a sample file which is provided by kafka, to show how kafka can be started without zookeeper Create 3 new files from server.properties. This is because we will be creating a 3 node cluster cd config/kraft cp server.properties server1.properties cp server.properties server2.properties cp server.properties server3.properties In server1.properties , modify the following properties. Please keep the other properties as is. node.id=1 process.roles=broker,controller inter.broker.listener.name=PLAINTEXT controller.listener.names=CONTROLLER listeners=PLAINTEXT://:9092,CONTROLLER://:19092 log.dirs=/tmp/server1/kraft-combined-logs listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL controller.quorum.voters=1@localhost:19092,2@localhost:19093,3@localhost:19094 Let me explain what these properties do: node.id : This will act as the node Id in the cluster. This will help us identify which broker this is. It will also help us identify which kraft controller node this is. process.roles : A node can act as a broker or controller or both. Here we are indicating that this node can be both a kafka broker and a kraft controller node. inter.broker.listener.name : Here the broker listener name is set to PLAINTEXT controller.listener.names : Here the controller listener name is set to CONTROLLER listeners : Here we indicate that the broker will use port 9092 and the kraft controller will use port 19092 log.dirs : This is the log directory where kafka will store the data listener.security.protocol.map : Here the connection security details are added controller.quorum.voters : This is used to indicate all the kraft controllers which are available. Here we are indicating that we will have 3 kraft controller nodes running on ports 19092, 19093 and 19094 For server2.properties modify the following properties. Please keep the other properties as is. node.id=2 process.roles=broker,controller controller.quorum.voters=1@localhost:19092,2@localhost:19093,3@localhost:19094 listeners=PLAINTEXT://:9093,CONTROLLER://:19093 inter.broker.listener.name=PLAINTEXT controller.listener.names=CONTROLLER log.dirs=/tmp/server2/kraft-combined-logs listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL In Server 2, the broker port is 9093 and controller port is 19093. Also the log.dirs is different For server3.properties modify the following properties. Please keep the other properties as is. node.id=3 process.roles=broker,controller controller.quorum.voters=1@localhost:19092,2@localhost:19093,3@localhost:19094 listeners=PLAINTEXT://:9094,CONTROLLER://:19094 inter.broker.listener.name=PLAINTEXT controller.listener.names=CONTROLLER log.dirs=/tmp/server3/kraft-combined-logs listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL In Server 3 the broker port is 9094 and the controller port is 19094. Also the log.dirs is different Kafka cluster id creation and log directory setup First we need to create kafka cluster id before starting the servers. This can be done using the following command: ./bin/kafka-storage.sh random-uuid Note down the uuid that comes after running the above statement. In my case I got the following uuid ./bin/kafka-storage.sh random-uuid 9dJzdGvfTPaCY4e8klXaDQ Next we need to format all the storage directories. This is basically the directory that we put in log.dirs property. This can be done with the following command ./bin/kafka-storage.sh format -t <uuid> -c <server_config_location> Replace <uuid> with the uuid that you got in the previous step. Replace <server_config_location> with the server property files In my case, I will be running the following commands For Server 1: ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server1.properties For Server 2: ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server2.properties For Server 3: ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server3.properties This is the result which you get when you run the 3 commands $ ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server1.properties Formatting /tmp/server1/kraft-combined-logs $ ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server2.properties Formatting /tmp/server2/kraft-combined-logs $ ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server3.properties Formatting /tmp/server3/kraft-combined-logs Starting the kafka servers The kafka servers can be started in daemon mode using the following commands: First setup the heap properties export KAFKA_HEAP_OPTS=\"-Xmx200M \u2013Xms100M\" Here we are giving a very small max heap of 200M since we are running all the servers in a single local machine. If you have bigger servers you can give heap size of 1GB or Above. Start Server 1: ./bin/kafka-server-start.sh -daemon ./config/kraft/server1.properties Start Server 2: ./bin/kafka-server-start.sh -daemon ./config/kraft/server2.properties Start Server 3: ./bin/kafka-server-start.sh -daemon ./config/kraft/server3.properties Create a kafka topic Let us create a topic kraft-test in this cluster The topic can be created using the following command: ./bin/kafka-topics.sh --create --topic kraft-test --partitions 3 --replication-factor 3 --bootstrap-server localhost:9092 We are creating a topic with 3 partitions and 3 replicas since we have 3 nodes. We can list the topics present in the cluster using the following command: bin/kafka-topics.sh --bootstrap-server localhost:9093 --list Running the above command gives the below result $ bin/kafka-topics.sh --bootstrap-server localhost:9093 --list kraft-test We can describe the topics present in the cluster using the following command: bin/kafka-topics.sh --bootstrap-server localhost:9093 --describe --topic kraft-test Running the above command gives the below result $ bin/kafka-topics.sh --bootstrap-server localhost:9093 --describe --topic kraft-test Topic: kraft-test TopicId: vZKswHHlQk2mEOw0yzAGAA PartitionCount: 3 ReplicationFactor: 3 Configs: segment.bytes=1073741824 Topic: kraft-test Partition: 0 Leader: 3 Replicas: 3,2,1 Isr: 3,2,1 Topic: kraft-test Partition: 1 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 Topic: kraft-test Partition: 2 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 Exploring the kafka metadata using metadata shell Similar to zookeeper cli, there is a metadata shell provided by kafka so that we can read the data in the @metadata internal topic. Open the shell using the following command ./bin/kafka-metadata-shell.sh --snapshot /tmp/server1/kraft-combined-logs/\\@metadata-0/00000000000000000000.log The structure here is very similar to what we see in zookeeper In order the list the brokers you can type ls brokers/ >> ls brokers/ 1 2 3 In order to list the topics you can type ls topics/ >> ls topics/ kraft-test In order to see the topic metadata you can type cat topics/kraft-test/0/data >> cat topics/kraft-test/0/data { \"partitionId\" : 0, \"topicId\" : \"vZKswHHlQk2mEOw0yzAGAA\", \"replicas\" : [ 3, 2, 1 ], \"isr\" : [ 3, 2, 1 ], \"removingReplicas\" : null, \"addingReplicas\" : null, \"leader\" : 3, \"leaderEpoch\" : 0, \"partitionEpoch\" : 0 } In order to find the controller leader node you can type cat metadataQuorum/leader >> cat metadataQuorum/leader MetaLogLeader(nodeId=2, epoch=4) Type exit to get out of the metadata shell Producing and consuming data from kafka Use the following command to start a kafka producer in the terminal bin/kafka-console-producer.sh --broker-list localhost:9092 --topic kraft-test In a different terminal, use the following command to start a kafka consumer bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kraft-test In the producer terminal send a bunch of messages as shown below $ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic kraft-test >message 1 >message 2 >message 3 >hello >bye In the consumer terminal you would see the messages coming as shown below bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kraft-test message 1 message 2 message 3 hello bye ref: adityasridhar.com","title":"Kafka"},{"location":"kafka/kafka/#install-kafka-without-zookeeper","text":"In this article we will see how kafka can be setup without using zookeeper. We will setup a 3 node kafka cluster and create a test topic. We will use a kafka producer to produce data into the test topic and also use a kafka consumer to consume data from the kafka topic.","title":"Install kafka without zookeeper"},{"location":"kafka/kafka/#why-is-zookeeper-used-in-kafka","text":"Zookeeper is used to store kafka cluster metadata information. Zookeeper stores information like topic configuration, topic partition locations and so on.","title":"Why is zookeeper used in kafka"},{"location":"kafka/kafka/#how-will-kafka-work-without-zookeeper","text":"Zookeeper acts as a external metadata management system for kafka. This creates multiple problems such as data duplication, increased system complexity and also leads to additional java processes being used. In order to run kafka without zookeeper, it can be run using Kafka Raft metadata mode ( KRaft ). In KRaft the kafka metadata information will be stored as a partition within kafka itself. There will be a KRaft Quorum of controller nodes which will be used to store the metadata. The metadata will be stored in an internal kafka topic @metadata . This is available in an experimental mode in kafka 2.8.0 Right now using KRaft is experimental and should not be used in production Setting up a kafka cluster without zookeeper Download kafka Download kafka 2.8.0 from https://kafka.apache.org/downloads wget https://apachemirror.wuchna.com/kafka/2.8.0/kafka_2.12-2.8.0.tgz Extract kafka tar xzf kafka_2.12-2.8.0.tgz Open the kafka folder. All kafka commands should be run in the kafka folder cd kafka_2.12-2.8.0 Kafka cluster configuration If you go to config/kraft folder inside the kafka home directory, you will see a file called server.properties. This is a sample file which is provided by kafka, to show how kafka can be started without zookeeper Create 3 new files from server.properties. This is because we will be creating a 3 node cluster cd config/kraft cp server.properties server1.properties cp server.properties server2.properties cp server.properties server3.properties In server1.properties , modify the following properties. Please keep the other properties as is. node.id=1 process.roles=broker,controller inter.broker.listener.name=PLAINTEXT controller.listener.names=CONTROLLER listeners=PLAINTEXT://:9092,CONTROLLER://:19092 log.dirs=/tmp/server1/kraft-combined-logs listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL controller.quorum.voters=1@localhost:19092,2@localhost:19093,3@localhost:19094 Let me explain what these properties do: node.id : This will act as the node Id in the cluster. This will help us identify which broker this is. It will also help us identify which kraft controller node this is. process.roles : A node can act as a broker or controller or both. Here we are indicating that this node can be both a kafka broker and a kraft controller node. inter.broker.listener.name : Here the broker listener name is set to PLAINTEXT controller.listener.names : Here the controller listener name is set to CONTROLLER listeners : Here we indicate that the broker will use port 9092 and the kraft controller will use port 19092 log.dirs : This is the log directory where kafka will store the data listener.security.protocol.map : Here the connection security details are added controller.quorum.voters : This is used to indicate all the kraft controllers which are available. Here we are indicating that we will have 3 kraft controller nodes running on ports 19092, 19093 and 19094 For server2.properties modify the following properties. Please keep the other properties as is. node.id=2 process.roles=broker,controller controller.quorum.voters=1@localhost:19092,2@localhost:19093,3@localhost:19094 listeners=PLAINTEXT://:9093,CONTROLLER://:19093 inter.broker.listener.name=PLAINTEXT controller.listener.names=CONTROLLER log.dirs=/tmp/server2/kraft-combined-logs listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL In Server 2, the broker port is 9093 and controller port is 19093. Also the log.dirs is different For server3.properties modify the following properties. Please keep the other properties as is. node.id=3 process.roles=broker,controller controller.quorum.voters=1@localhost:19092,2@localhost:19093,3@localhost:19094 listeners=PLAINTEXT://:9094,CONTROLLER://:19094 inter.broker.listener.name=PLAINTEXT controller.listener.names=CONTROLLER log.dirs=/tmp/server3/kraft-combined-logs listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL In Server 3 the broker port is 9094 and the controller port is 19094. Also the log.dirs is different Kafka cluster id creation and log directory setup First we need to create kafka cluster id before starting the servers. This can be done using the following command: ./bin/kafka-storage.sh random-uuid Note down the uuid that comes after running the above statement. In my case I got the following uuid ./bin/kafka-storage.sh random-uuid 9dJzdGvfTPaCY4e8klXaDQ Next we need to format all the storage directories. This is basically the directory that we put in log.dirs property. This can be done with the following command ./bin/kafka-storage.sh format -t <uuid> -c <server_config_location> Replace <uuid> with the uuid that you got in the previous step. Replace <server_config_location> with the server property files In my case, I will be running the following commands For Server 1: ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server1.properties For Server 2: ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server2.properties For Server 3: ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server3.properties This is the result which you get when you run the 3 commands $ ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server1.properties Formatting /tmp/server1/kraft-combined-logs $ ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server2.properties Formatting /tmp/server2/kraft-combined-logs $ ./bin/kafka-storage.sh format -t 9dJzdGvfTPaCY4e8klXaDQ -c ./config/kraft/server3.properties Formatting /tmp/server3/kraft-combined-logs Starting the kafka servers The kafka servers can be started in daemon mode using the following commands: First setup the heap properties export KAFKA_HEAP_OPTS=\"-Xmx200M \u2013Xms100M\" Here we are giving a very small max heap of 200M since we are running all the servers in a single local machine. If you have bigger servers you can give heap size of 1GB or Above. Start Server 1: ./bin/kafka-server-start.sh -daemon ./config/kraft/server1.properties Start Server 2: ./bin/kafka-server-start.sh -daemon ./config/kraft/server2.properties Start Server 3: ./bin/kafka-server-start.sh -daemon ./config/kraft/server3.properties Create a kafka topic Let us create a topic kraft-test in this cluster The topic can be created using the following command: ./bin/kafka-topics.sh --create --topic kraft-test --partitions 3 --replication-factor 3 --bootstrap-server localhost:9092 We are creating a topic with 3 partitions and 3 replicas since we have 3 nodes. We can list the topics present in the cluster using the following command: bin/kafka-topics.sh --bootstrap-server localhost:9093 --list Running the above command gives the below result $ bin/kafka-topics.sh --bootstrap-server localhost:9093 --list kraft-test We can describe the topics present in the cluster using the following command: bin/kafka-topics.sh --bootstrap-server localhost:9093 --describe --topic kraft-test Running the above command gives the below result $ bin/kafka-topics.sh --bootstrap-server localhost:9093 --describe --topic kraft-test Topic: kraft-test TopicId: vZKswHHlQk2mEOw0yzAGAA PartitionCount: 3 ReplicationFactor: 3 Configs: segment.bytes=1073741824 Topic: kraft-test Partition: 0 Leader: 3 Replicas: 3,2,1 Isr: 3,2,1 Topic: kraft-test Partition: 1 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 Topic: kraft-test Partition: 2 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 Exploring the kafka metadata using metadata shell Similar to zookeeper cli, there is a metadata shell provided by kafka so that we can read the data in the @metadata internal topic. Open the shell using the following command ./bin/kafka-metadata-shell.sh --snapshot /tmp/server1/kraft-combined-logs/\\@metadata-0/00000000000000000000.log The structure here is very similar to what we see in zookeeper In order the list the brokers you can type ls brokers/ >> ls brokers/ 1 2 3 In order to list the topics you can type ls topics/ >> ls topics/ kraft-test In order to see the topic metadata you can type cat topics/kraft-test/0/data >> cat topics/kraft-test/0/data { \"partitionId\" : 0, \"topicId\" : \"vZKswHHlQk2mEOw0yzAGAA\", \"replicas\" : [ 3, 2, 1 ], \"isr\" : [ 3, 2, 1 ], \"removingReplicas\" : null, \"addingReplicas\" : null, \"leader\" : 3, \"leaderEpoch\" : 0, \"partitionEpoch\" : 0 } In order to find the controller leader node you can type cat metadataQuorum/leader >> cat metadataQuorum/leader MetaLogLeader(nodeId=2, epoch=4) Type exit to get out of the metadata shell Producing and consuming data from kafka Use the following command to start a kafka producer in the terminal bin/kafka-console-producer.sh --broker-list localhost:9092 --topic kraft-test In a different terminal, use the following command to start a kafka consumer bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kraft-test In the producer terminal send a bunch of messages as shown below $ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic kraft-test >message 1 >message 2 >message 3 >hello >bye In the consumer terminal you would see the messages coming as shown below bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kraft-test message 1 message 2 message 3 hello bye ref: adityasridhar.com","title":"How will kafka work without zookeeper"},{"location":"linux/curl/","text":"Work in progress This document is a work in progress.","title":"Curl"},{"location":"linux/tips/","text":"Work in progress This document is a work in progress. ln -sf /usr/share/zoneinfo/Asia/Ho_Chi_Minh /etc/localtime sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config","title":"Tips"},{"location":"networking/iptables/","text":"Work in progress This document is a work in progress. XXXXXXXXXXXXXXXXXX XXX Network XXX XXXXXXXXXXXXXXXXXX + | v +-------------+ +------------------+ |table: filter| <---+ | table: nat | |chain: INPUT | | | chain: PREROUTING| +-----+-------+ | +--------+---------+ | | | v | v [local process] | **************** +--------------+ | +---------+ Routing decision +------> |table: filter | v **************** |chain: FORWARD| **************** +------+-------+ Routing decision | **************** | | | v **************** | +-------------+ +------> Routing decision <---------------+ |table: nat | | **************** |chain: OUTPUT| | + +-----+-------+ | | | | v v | +-------------------+ +--------------+ | | table: nat | |table: filter | +----+ | chain: POSTROUTING| |chain: OUTPUT | +--------+----------+ +--------------+ | v XXXXXXXXXXXXXXXXXX XXX Network XXX XXXXXXXXXXXXXXXXXX Tables \u00b6 iptables contains five tables: raw is used only for configuring packets so that they are exempt from connection tracking. filter is the default table, and is where all the actions typically associated with a firewall take place. nat is used for network address translation (e.g. port forwarding). mangle is used for specialized packet alterations. security is used for Mandatory Access Control networking rules.","title":"Iptables"},{"location":"networking/iptables/#tables","text":"iptables contains five tables: raw is used only for configuring packets so that they are exempt from connection tracking. filter is the default table, and is where all the actions typically associated with a firewall take place. nat is used for network address translation (e.g. port forwarding). mangle is used for specialized packet alterations. security is used for Mandatory Access Control networking rules.","title":"Tables"}]}